%% It is just an empty TeX file.
%% Write your code here.
\chapter{Communication System Test}\label{ch:experiment}

\section{Functional Shorts Implementation}
To verify the functional test concept an example communication system has been built and shown in \autoref{fig:short_ena}. The implementation lacks real modules, except from the encoder and decoder. Every other module is implemented as a simple delay in the digital information flow (flip flop) and XOR gate to conduct the error injection. They also have special enable signal to choose which module should occur as faulty. The AIR simulates the communication channel. There is also a programmable delay to variate the execution time of the test and to simulate the fact, that the number of clock cycles, needed for the information to pass from the encoder to decoder, is not constant in all tests. The short enable signals control the data flow in the test loop. The activation of any short, excludes the whole part of the system laying beyond the short and shortens the test duration, because the information doesn't need to be clocked through all excluded modules. The injected error vector travels through the design together with the test data. This guaranties the proper alignment of the error vector with the test vector at any time, also during the shortcut, when the path is shorter. The error vector flips the specified bits of the output of any chosen module to simulate the possible effects of hardware faults on the processed information. The error enable signal is exclusive, meaning that only one module may be faulty at the same time. 

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/Short_ena_err.png}
\caption{Implementation of functional shorts into the Test Bench}
\label{fig:short_ena}
\end{figure}

The experimental set-up has been implemented into the Test Bench. It consists of the BCH(1023,943) Encoder and Decoder, being able to correct up to 8 bit errors. The encoder and decoder need a reset signal to start working. The reset is followed by 943 random data bits and 80 zeros to compute the parity bits in the encoder. Parallel to these 1023 encoder bits, a 1023 bit error vector is shifted in. The bit that is supposed to be flipped is set to one, the rest to zero. The data block and the error vector are appended by the zero vector of the following length. The size of the programmable delay plus one for every module in the current test loop. The excluded modules laying beyond the active short, are not counted. The decoder needs additional 97 clock cycles for syndrome calculations and error localization, therefore another 97 zeros need to be added to the input signals. To shift out the corrected user bits another 943 cycles are needed. To monitor the error detection in the whole block, another 80 bits can be shifted out. The decoder response lies in the last 1023 bits of the output bitstream, so do 1023 special output bits, indicating the correction.

All tests revealed, that the method can be used for systematic fault diagnosis on the module level. Systematic exclusion of modules and test repetition allowed to localize a faulty module in all cases. The number of errors was lower or equal to the correction capability of the decoder. But how do the system react to the higher number of errors and what real errors occur as result of real permanent faults? Firstly the BCH encoder was tested for errors resulting from delay faults. Then fault injection was conducted to observe how the encoder reacts in presence of transient and permanent faults. Then the decoder was tested for error detection above its maximal correction capability and also for delay faults.

\section{Test of BCH(1023,943) Encoder}
The tested encoder is a BCH encoder taking 943 payload bits and calculating 80 parity bits. The encoder is actually a part of the PENCA architecture, without any redundant test and configuration circuitry. The encoder consists of an LFSR for parity calculations and the counter to keep track on the number of input user bits. Before the counter reaches 943 bits, it forwards the user bits to the output and enables linear feedback in the shift register. Then the parities are shifted out with "dummy" input bits with the LFSR acting as simple shift register. The parity calculation is obtained by dividing the input polynomial by the generator polynomial of the encoder. The reminder of the division is the parity polynomial. The general architecture of a BCH encoder using a LFSR for polynomial division is shown in \autoref{fig:enc_0}. Since the test bench developed during the project allows a test with wide range of frequencies, it may be possible to cause some delay faults in the encoder architecture and examine, how they influence the output bit stream.

\begin{figure}[h]
\centering
\includegraphics[width=0.65\textwidth]{figures/BCH_ENC.png}
\caption{A BCH encoder architecture \cite{art:BCH_implement}}
\label{fig:enc_0}
\end{figure}

The encoder has been placed in the test bench and tested for maximum frequency. The input data is random. There were 300 messages sent and evaluated. The comparison data was calculated using MATLAB 2015b comm.BCHEncoder. There were two encoder versions tested. One version is surrounded by flip-flops to guarantee that the delay fault happens in the encoder design and not due to the long connection paths between encoder design and the Test Engine. This version was called "isolated". The "not isolated" one was connected simply to the input and output registers of the Test Engine. First clock cycle is reserved for the reset. When the reset is low, the input vector is shifted in serially. The output is equal to the input when the payload bits are shifted in. After 943 payload bits, the input is set to zero and held that way till the end of the test, which means another 80 clock cycles for the parity bits to be shifted out. The test fails when the output value is different from the pre-calcualted value. The frequency is raised until first tests start to fail. This marks the cut-off frequency of the Encoder and reveals the errors caused by delay fault. The cut-off frequencies are presented in \autoref{tab:enc}.

\begin{table}[h]
\begin{tabular}{@{}lllll@{}}
\toprule
UUT                       &mem\_width   &uut\_width &expected freq. &max. freq.\\ 
\midrule
isolated Encoder                  & 32          & 4      & $\sim$642 MHz & $\sim$500 MHz \\
not isolated Encoder              & 32          & 4      & $\sim$642 MHz & $\sim$326 MHz \\
\bottomrule
\end{tabular}
\centering
\caption{Test Encoder for max. frequency}\label{tab:enc}
\end{table}

The result shows clearly, that a Test Engine limit of 500 MHz has been reached while testing the isolated Encoder, the frequency was not raised above this value, because the result would not be reliable. The not-isolated version on the other hand, failed by the frequency of 326 MHz. Both versions differ only in flip-flops isolating them from the test bench, therefore any errors happening in the not-isolated version did not arise in the Encoder itself. They originate from the long connections between the Encoder and the Test Bench. But such errors could also arise due to delay faults in real hardware. 

\autoref{fig:enc_1} represents the result of all failed test of the not-isolated Encoder at approx. 355 MHz. In half of the messages the first parity bit is corrupted. When the counter, calculating the number of shifted bits, reaches the 943, the output should be switched to the parity bits (see \autoref{fig:enc_0}). In all vectors, the output still forwards the input value, which is zero for all "dummy" parity bits. In half of the vectors, since they are randomly distributed, the zero is accidentally the expected value, therefore the error gets masked.

The raised frequency together with very long interconnect simulated a delay fault in the control signal of the multiplexer. The situtation is marked as red line in \autoref{fig:enc_0}. When the counter reaches 943, a special LCK signal is generated to disable feedback in the LFSR and change the source of the output pin to forward LFSR output instead of Encoders input. The outcome is a single bit error in half of the produced code words. This error can be easily detected and corrected by the decoder.

\begin{figure}[h]
\centering
\includegraphics[width=.65\textwidth]{figures/enc_error.png}
\caption{The result of failed Encoder test by 355 MHz}
\label{fig:enc_1}
\end{figure}



\section{Test of BCH(1023,943) Decoder}

For any positive integers $m \geq 3$ and $t<2^{m-1}$ there exists a binary BCH code with the following parameters \cite{art:BCH_implement}:
\begin{subequations}
\begin{align}
    \text{block length }n&=2^{m}-1\label{eq:blck_len}\\
    \text{parity bits }k&\geq n+mt\label{eq:parity}\\
    \text{hamming distance }d&\geq2t+1\label{eq:dmin}
\end{align}
\end{subequations}
The BCH(1023,943) with its $n = 1023$ and $k=943$, has a $d_{min}= 17$. The code is therefore capable of detecting 16 error bits and correcting 8 in a single block. 

The tested BCH decoder has one serial data input (DIN) and one data output (DOUT). The input data enters the syndrome computation block, which is similarly to encoder, implemented as LFSRs. It is actually less complicated then the encoder, having shorter LFSRs to compute partial syndromes, one for every correctable error. When all syndromes are equal zero, the received message is a valid code word. In other case, when any syndrome is non-zero, the Key Equation Solver has to find the parameters of error locating polynomial. The parameters are passed to Chien Search Block to find the roots of the error locating polynomial and therefore the exact positions of errors in the received message. The output of the Chien Search Block is connected to the output ISERR to indicate, if the current output bit has been repaired. This is the special extension allowing the Functional Short Concept to be implemented. The decoder needs 97 clock cycles before it starts shifting out the corrected user bits. The basic idea of building a BCH Decoder is shown in \autoref{fig:dec_0}.

\begin{figure}[h]
\centering
\includegraphics[width=0.65\textwidth]{figures/BCH_DEC.png}
\caption{A BCH decoder architecture \cite{art:BCH_implement}}
\label{fig:dec_0}
\end{figure}

\subsection{Test Decoder for maximal frequency}

The test procedure involving functional shortcuts assumed a fault free functionality of the decoder. It is possible though, that the decoder suffers from any permanent or transient fault itself, giving false negative or false positive results of the test, not mentioning the problems during normal communication. The decoder was tested for its maximal operational frequency and beyond. The decoder was tested only until its maximal correction capabilities, meaning 8 injected errors. The error injection happens statically during the data generation. The data is generated in steps. Firstly, 943 random bits are processed by the comm.BCHEncoder block in MATLAB. Then, the coded bitstream is XORed with an error vector with random error positions, but constant hamming weight, creating a data input for the decoder test. The expected result is the output of the MATLAB block, before error injection. The hamming weight of the error vector was increased from 1 to 8 and tested for maximum frequency. The results are summarized in \autoref{tab:dec}
\begin{table}[h]
\begin{tabular}{@{}ccccll@{}}
\toprule
UUT                       &mem\_width   &uut\_width &error cnt. &\begin{tabular}{@{}c@{}}unconstrained \\max. freq.\end{tabular} &\begin{tabular}{@{}c@{}}constrained \\max. freq.\end{tabular}\\ 
\midrule
\multirow{9}{*}{\begin{tabular}{@{}c@{}}isolated \\Decoder\end{tabular}}    & \multirow{9}{*}{32}        & \multirow{9}{*}{8}       &0           & $\sim$264 MHz & $\sim$319 MHz \\
                            &                           &                           &1          &   $\sim$237 MHz &   $\sim$229 MHz\\
                            &                           &                           &2          &   $\sim$200 MHz &   $\sim$221 MHz\\
                            &                           &                           &3          &   $\sim$200 MHz &   $\sim$219 MHz\\
                            &                           &                           &4          &   $\sim$200 MHz &   $\sim$219 MHz\\
                            &                           &                           &5          &   $\sim$180 MHz &   $\sim$218 MHz\\          
                            &                           &                           &6          &   $\sim$160 MHz &   $\sim$219 MHz\\
                            &                           &                           &7          &   $\sim$160 MHz &   $\sim$218 MHz\\
                            &                           &                           &8          &   $\sim$160 MHz &   $\sim$219 MHz\\
\bottomrule
\end{tabular}
\centering
\caption{Test Decoder for max. frequency}\label{tab:dec}
\end{table}

The isolated version means that the design was surrounded by flip-flops and the unconstrained design allows the tool to automatically place the design in the FPGA. The direct input and output is always compared with their delayed pairs to check, if the delay fault did not happen on the interface with the Test Bench. The isolation itself did not prevent delay faults on interconnects to manifest and influence the test results. When the frequency was raised above 300 MHz, the unconstrained decoder started to have errors on $DOUT$ output without errors on $ISERR$ output. This situtation is not possible, since the $ISERR$ signal is responsible for flipping the $DOUT$ bits. Hence the error happens in the decoders interface although the design was isolated with additional flip-flops. Both, the delayed and direct paths were erroneous, which prevented those faults from being detected. The reason was the design placement. In the unconstrained version, the palcing tool decided to place the isolating registers far away from the decoder and close to the Test Engine. This made the isolation useless and didn't allow to detect the delay faults happening on the interconnects with the Test Engine. Constraining the design was the only way to force the tool to make the isolating registers part of the decoder design and in consequence, to detect interconnect faults. The unconstrained design is presented in \autoref{subfig:uncon}, while the constrained design is showed in \autoref{subfig:con}.

\begin{figure}[h]
\centering
\begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/Device_edited.png}
    \caption{Unconstrained}
    \label{subfig:uncon}
\end{subfigure}%
\hspace{\fill}
\begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/Device_Pblock_edited.png}
    \caption{Constrained}
    \label{subfig:con}
\end{subfigure}
\caption{Decoder placement in FPGA}
\label{fig:dec_routing}
\end{figure}

The PBLOCKS are areas in the FPGA, that force the placement tool to try to fit parts of the design inside of them. After drawing the PBLOCK between the old decoder position and the test engine, the decoder was placed closer to the test engine, so the delay fault at the interconnect should appear later and the delayed and direct signal should differ. The detection of delay faults at interconnects is now possible. The tests of constrained design showed that the maximal frequencies were higher, because the distances between various cells were smaller. The design has been closed in smaller space, making the circuit more dense and the cells were repositioned. The maximal frequencies of the constrained design have been presented in \autoref{tab:dec}.

The XML file returned by the test engine allows to examine the failed tests and see where the errors occur and in what number. Every test at cut-off frequency simulates some delay faults and shows the misbehavior of the longest paths in the design. The first test was conducted on isolated constrained design of the decoder. There were 1000 code words sent and every code word had one injected error. At normal frequency, this amount of errors is easily correctable by the decoder, but at the cut-off frequency, the decoder starts to produce some errors by itself. The cut-off frequency for the decoder with 1 injected error is 229 MHz. The test was conducted with 230 MHz. The result of the test is summarized in 3 subfigures in \autoref{fig:dec_3}. 


\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/1000_tests_decoder_overtacted_230_1_error.png}
\caption{Test of BCH Decoder with 1 injected error over the cut-off frequency}
\label{fig:dec_3}
\end{figure}

The most left hand-side subfigure represents the user bits of the received code word, containing 1000 received code word, each 943 bits long. The middle figure represents the received parity bits 1000 code words with 80 parities. Every rectangle with a dot represents an error that the decoder produced by itself. The expected results were calculated using the MATLAB 2015b comm.BCHDecoder.

The error distribution is random, data dependent. For the decoder to produce an error, either the syndrome calculation failed or the Key Equation Solver or Chien Search Block. The single syndrome computation is very similar to the parity computation in the encoder, since it also involves LFSR. The encoder test revealed, that a LFSR doesn't fail at such low frequency, hence the errors are produced somewhere in the Key Solver or Chien Search Block. If the Chien Search Block failed, it could produce more errors, then the theoretical decoder correction capability. Such thing was not visible during any of the tests. The fault is therefore probably in the Key Equation Solver or in the control logic.

Another interesting problem is the behavior of the decoder when no errors are injected. During the cut-off frequency test, the maximal frequency for isolated constrained decoder was 319 MHz. After sending 1000 valid code words, with no errors injected and raisng the test frequency to 320 MHz, the decoder started to produce rather repeatable errors. The result is showed in \autoref{fig:dec_2}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/1000_tests_0_faults_320_MHz_1.png}
\caption{Test of BCH Decoder with 0 injected errors over the cut-off frequency}
\label{fig:dec_2}
\end{figure}

In the test with no injected errors, the syndromes and therefore the parameters of error locator polynomial should be zero at all times. The syndromes calculated with use of LFSR should be correct at this frequency. Again the Key Equation Solver is probably responsible for this error patterns.


\subsection{Test Decoder for Maximal Error Detection}

In the Functional Short Test, only the ISERR output is monitored, since the data is irrelevant. In normal function, the decoder returns only 943 corrected user bits. Upon detection of an error in the parity bits only, no correction is necessary, since the user bits are not changed. In the tested decoder however, the information about erroneous parity bits is easily accessible by monitoring the $ISERR$ output for another 80 clock cycles. The observation of 80 check bits is crucial in error detection, since the whole vector travels through the tested design and can be disturbed in any position, not only in the user bit space. Loosing the information about the 80 check bits, would make some errors undetectable, like all errors happening inside of an encoder, which happen only to the parity bits. 

The underlaying BCH code is capable of correcting 8 bit errors and detecting 16. The blue bars in \autoref{fig:dec_1} show a result of a simple test, where randomly generated data was "XORed" with randomly generated error vectors with raising hamming weight. Each test consists of 1000 messages and the error number is raised from 1 to 18. The decoder successfully detects and corrects all errors in those messages, where the number of errors does not exceed 8. It fails to detect any higher number of errors, giving a detection rate of about 63\%. A successful detection means that the code word was not simply accepted as fault-free and some sort of correction took place. The corrected bits don't necessarily correspond to the injected errors for error count above 8. The error vector simply transforms code words into invalid vectors laying somewhere within correction spheres of other code words. The miscorrection takes place, which transforms the vectors into invalid code words. The miscorrection is indistinguishable from normal correction and is not reported by the decoder, which is a disadvantage during the normal function. Since the BCH code is not a perfect code, the distances between neighboring code words are sometimes bigger then the minimal hamming distance of the code. The implication is that there is a possibility that a code word gets transformed by an error vector into an invalid vector laying outside of all error correcting spheres. Such situtation is not detected and the invalid vector gets accepted as a fault-free code word. The inability of error detection up to 16 bits shows that the decoder was implemented only for error correction purposes and does not present the full functionality of the underlaying code.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/1000_tests_error_detection.png}
\caption{The error detection limits of the decoder}
\label{fig:dec_1}
\end{figure}

Because the Test Engine allows tests with higher frequencies and the cut-off frequency of the decoder is known to be 229 MHz, the decoder was tested at 230 MHz for its maximal error detection. The result is shown in \autoref{fig:dec_1}. Since the cut-off frequency is lower for higher error counts, another test was conducted at 220 MHz, to show how the error detection of the decoder changes together with the frequency.

The error detection capability of the decoder was diminished by delay faults happening at the cut-off frequency and above. It fails to reliably detect any number of errors, having a 99\% detection rate of single bit errors and 82\% for double bit errors.

The error correction is also dependent on frequency and gets diminished at cut-off frequency. The result of the degradation is visible in \autoref{fig:dec_2}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/1000_tests_error_detection.png}
\caption{The error detection limits of the decoder}
\label{fig:dec_1}
\end{figure}


The decoder circuit is unable to detect more errors then it can correct. Additionally, in case of permanent faults in the decoder, special BIST techniques need to be incorporated, since the faults will have influence on the results of the diagnostic test. Every permanent fault lowers the correcting and detecting capability of the decoder.

\chapter{Summary}

The diagnostic test is limited by the error detection capability of the decoder. The more errors a decoder can detect, the better chance of reliable test result. In systems with SEC-DED codes, like those implementing extended Hamming-code or Hsiao-code, the common weak point is erroneous error correction in case of triple bit errors and multiple bit errors \cite{art:Dicorato}. This weak point does not influence the Functional Short Test, since only the error detection counts. In case of Hsiao Code, any odd number of errors is detectable, which makes it a great candidate for detecting multiple bit errors during the test. The exact error positions are not mandatory in case of a simple fail-pass test on module level. In case of PENCA, the highest possible error correcting code should be used during the test.

In communication systems, where data is processed serially, like in BCH encoder and decoder tested in this thesis, the occurrence of multiple bit flips in case of a single fault is very high. Both the decoder and encoder are based on LFSRs, which need to be protected against single event upsets. 

Another limitation to the diagnostic test solution is the coverage of only those modules, which lay between encoder and decoder. The source encoding and cryptographic encryption are only two examples of modules, that may reside outside of the test subsystem, not covered by the diagnostic test. For those modules, well established digital test methods need to be applied. The same restriction applies to the decoder architecture itself. It has been admittedly reported, that some decoder architectures, like Hsiao-code decoder are capable of correcting (masking) some of its own hardware faults happening in the embedded encoder structure, under condition that they don't overlap with the transmission errors \cite{art:Dicorato}, but this feature does not cover the entire decoder structure and lowers the error coverage of the deocder. In the BCH decoder if any syndrome is non-zero a correction is triggered, so any fault in syndrome generation unit, that leads to a failure of the unit, will trigger a false correction, therefore an error in output bit stream. The same applies to the Key Solver unit or Chien Search Block. In all cases, the diagnostic test doesn't cover the decoder internal faults so the decoder dependability needs to be taken care of separately. The diagnostic test can help in finding encoder or decoder faulty, since the last shortcut tests only those two modules. In case of failed test, as in all other cases, the repair of one unit is conducted and the test is repeated.

Moreover the diagnostic test is used only for error detection. The repair of permanent faults require further redundancy, like spare units, leading to duplication of the hardware or parts of it. In case of patent protected circuitry, with no additional DfT extensions, a full duplication of hardware seems to be the only way to ensure repair possibility. Thanks to the diagnostic test, there is no need for trippling the hardware to create a TMR system.

The additional multiplexers, laying on transmission path, lead to signal deterioration, which is especially important in the analog part. The multiplexers need to be adjusted, so that their response does not disturb "normal" functionality of the system. In digital logic, the additional multiplexers create additional delay, although the switching happens off-line, so it should not influence the "normal" function.

The suggested diagnostic test requires also control logic to conduct the "fake" start-up procedure and maybe run some additional test vectors through the design to detect faults not visible during the start-up phase. Most of it can reside in software, since the test uses "normal" data path. The only signals needed during the implementation were the $ISERR$ output of the decoder and control signals to enable shortcuts. To conduct the repair, additional control inputs need to be provided.

In conclusion, the suggested solution is useful as diagnostic test in systems with no internal access to some modules laying between FEC encoder and decoder and allows the test of analog, mixed-signal and digital modules indifferently with just little hardware and software overhead. The test is limited by error detection capability of the decoder. The limitation is not different then the test result evaluation using signatures. The tests are false positive in case of error vectors changing one code word into another valid code word, just like conflicts happening in signature calculation. To extend the test to all modules in transceiver and receiver, more shortcuts can be added, between all corresponding modules, and the response analysis may happen in the software. This approach may be triggered when the proposed diagnostic test fails its purpose.

The results of tests conducted on encoder and decoder architectures, helped by the development process and were a valuable input in understanding the limitations of the designs. They also validated the proper functionality of the basic PENCA building blocks and showed, in case of decoder, where are the possible bottlenecks. Unfortunately, the test engine was not successful in testing the limits of encoder architecture.

As a result of this thesis, a useful tool for design validation in "real" hardware was developed and may be used as an alternative to simulation, speeding up the development process and allowing quick evaluation of responses with any chosen program. The placement of the design is to be treated with much caution and understanding, to avoid misinterpretation of test results. 